# üß† Deep Dive: How LLMs Generate Text

In Canopy AI, your assistant appears to think quickly and respond naturally. But what‚Äôs actually happening behind the scenes? This chapter explores the core mechanics that power LLMs during **inference** ‚Äî the process of turning prompts into meaningful, human-like output.

We'll focus on the concepts that matter most for real-world deployment in educational settings.

---

## üß± What is a Token?

Tokens are the **smallest units of text** an LLM processes. A token might be a word, a piece of a word, or even punctuation.

Examples:
- `"The"` ‚Üí 1 token
- `"unbelievable"` ‚Üí 3 tokens (`"un"`, `"believ"`, `"able"`)

LLMs don‚Äôt read full sentences‚Äîthey process token sequences. The total number of tokens affects:
- Memory usage
- Inference speed
- Output length

‚ö†Ô∏è **Tip**: More tokens = slower, more expensive inference.

<iframe
	src="https://agents-course-the-tokenizer-playground.static.hf.space"
	frameborder="0"
	width="500"
	height="750"
	style="border: 1px solid #ccc; border-radius: 8px;"
	loading="lazy">
></iframe>

*The App is from [HuggingFace Learning Course](https://agents-course-the-tokenizer-playground.static.hf.space)*

## üîÆ Are LLMs Fixed or Do They Change Over Time?

LLMs are **frozen once trained**‚Äîthey do **not learn** or update on the fly. Each time you send a prompt, they respond based on **pretrained knowledge** and context in the prompt.

However, outputs may differ due to:
- **Random sampling strategies**
- **Changes in prompts**
- **Different system instructions**

You can‚Äôt ‚Äúteach‚Äù an LLM new facts mid-conversation unless it‚Äôs part of the prompt or a fine-tuned model.

---

> TODO: With Gradio, send a message talking about "What did you learned today?". Send a new message, and say "What I did say in the last message?"

> TODO: Some text about memory, go to the canopy UI (final product) and do the same conversation again to see the memory.

> TODO: Ask the same questions again and see if you have the same answers.

## üîÑ Next-Token Prediction: How LLMs Work

LLMs are **next-token machines**. At their core, they do one thing:  
üëâ Predict the most likely next token based on everything they‚Äôve seen so far.

For example:
> Input: "Photosynthesis is the process by which plants"  
> Prediction: `" convert sunlight into energy"`

This generation happens one token at a time, using **probabilities** and **context** to decide what comes next.

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üìù Quiz: What does an LLM do during inference?</h3>

<style>
.quiz-container { position: relative; }
.quiz-option {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio { display: none; }
.quiz-radio:checked + .quiz-option { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio[value="wrong"]:checked + .quiz-option { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#correct:checked ~ .feedback.success { opacity: 1; }
#wrong1:checked ~ .feedback.error, #wrong2:checked ~ .feedback.error { opacity: 1; }
.feedback.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container">

  <input type="radio" name="quiz" id="wrong2" class="quiz-radio" value="wrong">
  <label for="wrong2" class="quiz-option">üóÑÔ∏è Retrieve facts from a database</label>

  <input type="radio" name="quiz" id="correct" class="quiz-radio" value="correct">
  <label for="correct" class="quiz-option" data-correct="true">üéØ Predict the most likely next token based on previous ones</label>
  
  <input type="radio" name="quiz" id="wrong1" class="quiz-radio" value="wrong">
  <label for="wrong1" class="quiz-option">üìä Classify the topic of a sentence</label>

  <div class="feedback success">‚úÖ <strong>Excellent!</strong> You understand how LLMs work during inference.</div>
  <div class="feedback error">‚ùå <strong>Try again!</strong> Think about what LLMs fundamentally do during text generation.</div>
</div>

</div>

## üëÄ What is Attention?

**Attention** helps the model focus on the most relevant tokens in the input when generating output.

In the sentence:
> "When the student finished the exam, they felt relieved."

To predict "they", the model uses attention to relate it back to "the student".

Attention is why LLMs feel smart ‚Äî it allows them to **track meaning and reference across long inputs**.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Visual Gif of Attention" width="60%">

*The video is from [HuggingFace Learning Course](https://huggingface.co/learn/llm-course/en/chapter1/8?fw=pt#the-role-of-attention)* 


<div class="iframe-scroll-container" style="width: 100%; overflow-x: auto; margin: 20px 0;">
  <iframe
    src="https://yatheshr-bert-attention-visualizer.hf.space"
    width="100%"
    height="600px"
    frameborder="0"
    style="border: 1px solid #ddd; border-radius: 8px; min-width: 800px;"
    loading="lazy"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    sandbox="allow-scripts allow-same-origin allow-forms allow-popups allow-presentation">
  </iframe>
</div>

<p><em>Interactive attention visualization tool from <a href="https://huggingface.co/spaces/Yatheshr/bert_attention_visualizer" target="_blank">HuggingFace Spaces</a>. This tool helps visualize how BERT models use attention mechanisms to focus on different parts of the input text.</em></p>


## üß† What is Context Length / Context Window?

The **context window** is how many tokens the model can ‚Äúremember‚Äù at once.

Typical ranges:
- Small models: 2K‚Äì4K tokens
- Modern models: 8K‚Äì128K+ tokens
- Cutting-edge models: up to 1 million tokens (e.g., Qwen2.5-1M)

More context = better understanding of long documents or prior messages.  
But it comes at a cost:
- Slower performance
- More VRAM usage
- Higher latency

> TODO: Gradio App with a very short context window. 

> TODO: Add a challenge to summarize some content that hits the context window max capacity

## ‚ö° KV Cache: Making Inference Faster

As models generate tokens, they keep track of past computations using a **KV (Key-Value) Cache**.

Instead of recomputing attention for every previous token at each step, the model stores the intermediate results (keys and values) from earlier layers and reuses them as it continues generating.

![KV Cache Autoregression Diagram - Shows how key-value caching optimizes token generation by storing and reusing previous computations](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/autoregression.png "KV Cache optimization during autoregressive text generation")

*Figure: KV Cache optimization during autoregressive text generation. Source: [HuggingFace Documentation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/autoregression.png)*

Benefits:
- Avoids repeating expensive calculations
- Greatly improves decode speed
- Reduces latency for long responses
- Enables responsive UIs like Canopy AI‚Äôs streaming assistant

KV caching is why you see real-time streaming after the first token is generated‚Äîit drastically reduces the work needed per new token, especially for longer conversations or document processing.

## üí° Prompting: How to Guide the Model

Prompting is how we shape model behavior. There are two key parts:
- **System Prompt**: Defines the assistant‚Äôs role (e.g., ‚ÄúYou are a helpful tutor‚Ä¶‚Äù)
- **User Prompt**: The actual input or question

Small changes in wording can **dramatically** change the output. That‚Äôs why prompt engineering is crucial in education ‚Äî it determines how clearly, accurately, and appropriately the model responds to students and instructors.

üìö We‚Äôll dive much deeper into **prompt engineering strategies**, including real examples and hands-on practice, in the next chapters.

## üö® Hallucinations: When Models Make Stuff Up

LLMs sometimes **hallucinate** ‚Äî confidently generate text that‚Äôs incorrect or fictional.

Why it happens:
- They optimize for coherence, not factual accuracy
- They don't ‚Äúknow‚Äù facts‚Äîthey predict likely token sequences

Mitigation tips:
- Include accurate facts in the prompt
- Use guardrails (see below)
- Add retrieval or validation layers

## üõ°Ô∏è Guardrails: Controlling What Models Say

To keep your assistant safe and on-task, you can apply **guardrails**, such as:
- Prompt templates with strict instructions
- Output filters (block offensive or harmful content)
- External validation (e.g., fact-checking or classifiers)

For Canopy AI, these guardrails are essential to ensure alignment with educational standards.

## üìè Key Inference Metrics

Understanding how your model performs helps you scale and troubleshoot.

| Metric                  | Meaning                                                      |
|-------------------------|--------------------------------------------------------------|
| **TTFT**                | Time to First Token ‚Äì how fast the model starts responding   |
| **TPOT**                | Time Per Output Token ‚Äì how fast each new token is generated |
| **Throughput**          | Number of parallel requests handled                          |
| **VRAM Usage**          | GPU memory required (‚Üë model size or context = ‚Üë memory)     |

These metrics help you balance latency vs. cost in OpenShift AI deployments.

## üì¶ Model Sizes and GPU Needs

Model size matters‚Äîfor performance *and* capability.

| Model Size     | Parameters | GPU Requirement            | Notes                            |
|----------------|------------|-----------------------------|----------------------------------|
| **<3B**         | Small      | 8‚Äì12GB VRAM (1 GPU)         | Lightweight and fast             |
| **7B‚Äì13B**      | Medium     | ‚â•24GB VRAM or quantization  | Balanced power vs. cost          |
| **>30B**        | Large      | Multi-GPU or high-end cards | Slower, but more context-aware   |

üß† Larger models may be smarter, but smaller ones are often faster and easier to deploy.

## ‚úÖ Summary

| Concept                | Key Idea                                                            |
|------------------------|---------------------------------------------------------------------|
| **Token**              | The basic unit of LLM input/output                                  |
| **Next-token machine** | LLMs predict one token at a time                                    |
| **Attention**          | Helps models focus on relevant words                                |
| **Context length**     | How much the model can "remember"                                   |
| **KV Cache**           | Speeds up generation by caching internal state                      |
| **Prompting**          | Guides model behavior through smart input design                    |
| **Hallucination**      | LLMs can generate plausible but wrong info                          |
| **Guardrails**         | Techniques to constrain model behavior and output                   |
| **TTFT & TPOT**        | Speed metrics for user experience                                   |
| **VRAM & Throughput**  | Resource and scalability metrics                                    |
| **Model size & GPU**   | Match model size to hardware capability and use case                |

---

## üìö References

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information processing systems*, 30. This seminal paper introduced the Transformer architecture and self-attention mechanism that forms the foundation of modern large language models like GPT, BERT, and others used in AI applications today.

