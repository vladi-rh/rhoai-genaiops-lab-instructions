# üß† Deep Dive: How LLMs Generate Text

In Canopy AI, your assistant appears to think quickly and respond naturally. But what‚Äôs actually happening behind the scenes? This chapter explores the core mechanics that power LLMs during **inference** ‚Äî the process of turning prompts into meaningful, human-like output.

We'll focus on the concepts that matter most for real-world deployment in educational settings.

---

## üß± What is a Token?

Tokens are the **smallest units of text** an LLM processes. A token might be a word, a piece of a word, or even punctuation.

Examples:
- `"The"` ‚Üí 1 token
- `"unbelievable"` ‚Üí 3 tokens (`"un"`, `"believ"`, `"able"`)

LLMs don't read full sentences‚Äîthey process token sequences. The total number of tokens affects:
- Memory usage
- Inference speed
- Output length

‚ö†Ô∏è **Tip**: More tokens = slower, more expensive inference.

<iframe
	src="https://agents-course-the-tokenizer-playground.static.hf.space"
	frameborder="0"
	width="500"
	height="750"
	style="border: 1px solid #ccc; border-radius: 8px;"
	loading="lazy">
></iframe>

*The App is from [HuggingFace Learning Course](https://agents-course-the-tokenizer-playground.static.hf.space)*

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üî§ Quiz: How do tokens impact LLM performance?</h3>

<style>
.quiz-container-tokens { position: relative; }
.quiz-option-tokens {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-tokens:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-tokens { display: none; }
.quiz-radio-tokens:checked + .quiz-option-tokens { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-tokens[value="wrong"]:checked + .quiz-option-tokens { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-tokens {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#tokens-correct:checked ~ .feedback-tokens.success { opacity: 1; }
#tokens-wrong1:checked ~ .feedback-tokens.error, #tokens-wrong2:checked ~ .feedback-tokens.error { opacity: 1; }
.feedback-tokens.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-tokens.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-tokens">

  <input type="radio" name="quiz-tokens" id="tokens-wrong1" class="quiz-radio-tokens" value="wrong">
  <label for="tokens-wrong1" class="quiz-option-tokens">üöÄ More tokens always improve output quality</label>

  <input type="radio" name="quiz-tokens" id="tokens-correct" class="quiz-radio-tokens" value="correct">
  <label for="tokens-correct" class="quiz-option-tokens" data-correct="true">‚ö° More tokens increase memory usage and slow down inference</label>
  
  <input type="radio" name="quiz-tokens" id="tokens-wrong2" class="quiz-radio-tokens" value="wrong">
  <label for="tokens-wrong2" class="quiz-option-tokens">üî¢ Token count doesn't affect processing speed</label>

  <div class="feedback-tokens success">‚úÖ <strong>Perfect!</strong> You understand that tokens directly impact performance and costs.</div>
  <div class="feedback-tokens error">‚ùå <strong>Not quite!</strong> Remember: more tokens = more memory + slower processing.</div>
</div>

</div>

## üîÆ Are LLMs Fixed or Do They Change Over Time?

LLMs are **frozen once trained**‚Äîthey do **not learn** or update on the fly. Each time you send a prompt, they respond based on **pretrained knowledge** and context in the prompt.

However, outputs may differ due to:
- **Random sampling strategies**
- **Changes in prompts**
- **Different system instructions**

You can't "teach" an LLM new facts mid-conversation unless it's part of the prompt or a fine-tuned model.

So let's do a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üß† Quiz: Do LLMs learn new information during conversations?</h3>

<style>
.quiz-container-learning { position: relative; }
.quiz-option-learning {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-learning:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-learning { display: none; }
.quiz-radio-learning:checked + .quiz-option-learning { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-learning[value="wrong"]:checked + .quiz-option-learning { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-learning {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#learning-correct:checked ~ .feedback-learning.success { opacity: 1; }
#learning-wrong1:checked ~ .feedback-learning.error, #learning-wrong2:checked ~ .feedback-learning.error { opacity: 1; }
.feedback-learning.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-learning.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-learning">

  <input type="radio" name="quiz-learning" id="learning-wrong1" class="quiz-radio-learning" value="wrong">
  <label for="learning-wrong1" class="quiz-option-learning">üìö Yes, they continuously update their knowledge from each conversation</label>

  <input type="radio" name="quiz-learning" id="learning-wrong2" class="quiz-radio-learning" value="wrong">
  <label for="learning-wrong2" class="quiz-option-learning">üîÑ They learn gradually but only remember within the same session</label>
  
  <input type="radio" name="quiz-learning" id="learning-correct" class="quiz-radio-learning" value="correct">
  <label for="learning-correct" class="quiz-option-learning" data-correct="true">üîí No, they are frozen after training and don't learn new information</label>

  <div class="feedback-learning success">‚úÖ <strong>Exactly right!</strong> LLMs are fixed after training and only work with their pretrained knowledge plus current context.</div>
  <div class="feedback-learning error">‚ùå <strong>Think again!</strong> LLMs don't update or learn - they're frozen after training.</div>
</div>

</div>

---

> TODO: With Gradio, send a message talking about "What did you learned today?". Send a new message, and say "What I did say in the last message?"

> TODO: Some text about memory, go to the canopy UI (final product) and do the same conversation again to see the memory.

> TODO: Ask the same questions again and see if you have the same answers.

## üîÑ Next-Token Prediction: How LLMs Work

LLMs are **next-token machines**. At their core, they do one thing:  
üëâ Predict the most likely next token based on everything they‚Äôve seen so far.

For example:
> Input: "Photosynthesis is the process by which plants"  
> Prediction: `" convert sunlight into energy"`

This generation happens one token at a time, using **probabilities** and **context** to decide what comes next.

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üìù Quiz: What does an LLM do during inference?</h3>

<style>
.quiz-container { position: relative; }
.quiz-option {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio { display: none; }
.quiz-radio:checked + .quiz-option { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio[value="wrong"]:checked + .quiz-option { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#correct:checked ~ .feedback.success { opacity: 1; }
#wrong1:checked ~ .feedback.error, #wrong2:checked ~ .feedback.error { opacity: 1; }
.feedback.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container">

  <input type="radio" name="quiz" id="wrong2" class="quiz-radio" value="wrong">
  <label for="wrong2" class="quiz-option">üóÑÔ∏è Retrieve facts from a database</label>

  <input type="radio" name="quiz" id="correct" class="quiz-radio" value="correct">
  <label for="correct" class="quiz-option" data-correct="true">üéØ Predict the most likely next token based on previous ones</label>
  
  <input type="radio" name="quiz" id="wrong1" class="quiz-radio" value="wrong">
  <label for="wrong1" class="quiz-option">üìä Classify the topic of a sentence</label>

  <div class="feedback success">‚úÖ <strong>Excellent!</strong> You understand how LLMs work during inference.</div>
  <div class="feedback error">‚ùå <strong>Try again!</strong> Think about what LLMs fundamentally do during text generation.</div>
</div>

</div>

## üëÄ What is Attention?

**Attention** helps the model focus on the most relevant tokens in the input when generating output.

In the sentence:
> "When the student finished the exam, they felt relieved."

To predict "they", the model uses attention to relate it back to "the student".

Attention is why LLMs feel smart ‚Äî it allows them to **track meaning and reference across long inputs**.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Visual Gif of Attention" width="60%">

*The video is from [HuggingFace Learning Course](https://huggingface.co/learn/llm-course/en/chapter1/8?fw=pt#the-role-of-attention)*


<div class="iframe-scroll-container" style="width: 100%; overflow-x: auto; margin: 20px 0;">
  <iframe
    src="https://yatheshr-bert-attention-visualizer.hf.space"
    width="100%"
    height="600px"
    frameborder="0"
    style="border: 1px solid #ddd; border-radius: 8px; min-width: 800px;"
    loading="lazy"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    sandbox="allow-scripts allow-same-origin allow-forms allow-popups allow-presentation">
  </iframe>
</div>

<p><em>Interactive attention visualization tool from <a href="https://huggingface.co/spaces/Yatheshr/bert_attention_visualizer" target="_blank">HuggingFace Spaces</a>. This tool helps visualize how BERT models use attention mechanisms to focus on different parts of the input text.</em></p>

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üëÄ Quiz: What does attention help LLMs do?</h3>

<style>
.quiz-container-attention { position: relative; }
.quiz-option-attention {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-attention:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-attention { display: none; }
.quiz-radio-attention:checked + .quiz-option-attention { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-attention[value="wrong"]:checked + .quiz-option-attention { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-attention {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#attention-correct:checked ~ .feedback-attention.success { opacity: 1; }
#attention-wrong1:checked ~ .feedback-attention.error, #attention-wrong2:checked ~ .feedback-attention.error { opacity: 1; }
.feedback-attention.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-attention.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-attention">

  <input type="radio" name="quiz-attention" id="attention-wrong1" class="quiz-radio-attention" value="wrong">
  <label for="attention-wrong1" class="quiz-option-attention">üéØ Focus only on the most recent tokens in the sequence</label>

  <input type="radio" name="quiz-attention" id="attention-correct" class="quiz-radio-attention" value="correct">
  <label for="attention-correct" class="quiz-option-attention" data-correct="true">üîó Focus on relevant tokens throughout the input to understand relationships</label>
  
  <input type="radio" name="quiz-attention" id="attention-wrong2" class="quiz-radio-attention" value="wrong">
  <label for="attention-wrong2" class="quiz-option-attention">üìä Calculate mathematical operations between tokens</label>

  <div class="feedback-attention success">‚úÖ <strong>Great job!</strong> Attention helps models understand which parts of the input are most relevant for generating each output token.</div>
  <div class="feedback-attention error">‚ùå <strong>Try again!</strong> Think about how attention helps models understand relationships across the entire input.</div>
</div>

</div>


## üß† What is Context Length / Context Window?

The **context window** is how many tokens the model can "remember" at once.

Typical ranges:
- Small models: 2K‚Äì4K tokens
- Modern models: 8K‚Äì128K+ tokens
- Cutting-edge models: up to 1 million tokens (e.g., Qwen2.5-1M)

More context = better understanding of long documents or prior messages.
But it comes at a cost:
- Slower performance
- More VRAM usage
- Higher latency

> TODO: Gradio App with a very short context window.

> TODO: Add a challenge to summarize some content that hits the context window max capacity

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üß† Quiz: What happens with larger context windows?</h3>

<style>
.quiz-container-context { position: relative; }
.quiz-option-context {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-context:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-context { display: none; }
.quiz-radio-context:checked + .quiz-option-context { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-context[value="wrong"]:checked + .quiz-option-context { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-context {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#context-correct:checked ~ .feedback-context.success { opacity: 1; }
#context-wrong1:checked ~ .feedback-context.error, #context-wrong2:checked ~ .feedback-context.error { opacity: 1; }
.feedback-context.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-context.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-context">

  <input type="radio" name="quiz-context" id="context-wrong1" class="quiz-radio-context" value="wrong">
  <label for="context-wrong1" class="quiz-option-context">üöÄ Models run faster and use less memory</label>

  <input type="radio" name="quiz-context" id="context-wrong2" class="quiz-radio-context" value="wrong">
  <label for="context-wrong2" class="quiz-option-context">üìà Only output quality improves with no downsides</label>
  
  <input type="radio" name="quiz-context" id="context-correct" class="quiz-radio-context" value="correct">
  <label for="context-correct" class="quiz-option-context" data-correct="true">‚ö° Better understanding but slower performance and higher VRAM usage</label>

  <div class="feedback-context success">‚úÖ <strong>Spot on!</strong> Larger context windows provide better understanding but come with performance and resource trade-offs.</div>
  <div class="feedback-context error">‚ùå <strong>Not quite!</strong> Remember: larger context = better understanding but more resources needed.</div>
</div>

</div>

## ‚ö° KV Cache: Making Inference Faster

As models generate tokens, they keep track of past computations using a **KV (Key-Value) Cache**.

Instead of recomputing attention for every previous token at each step, the model stores the intermediate results (keys and values) from earlier layers and reuses them as it continues generating.

![KV Cache Autoregression Diagram - Shows how key-value caching optimizes token generation by storing and reusing previous computations](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/autoregression.png "KV Cache optimization during autoregressive text generation")

*Figure: KV Cache optimization during autoregressive text generation. Source: [HuggingFace Documentation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/autoregression.png)*

Benefits:
- Avoids repeating expensive calculations
- Greatly improves decode speed
- Reduces latency for long responses
- Enables responsive UIs like Canopy AI's streaming assistant

KV caching is why you see real-time streaming after the first token is generated‚Äîit drastically reduces the work needed per new token, especially for longer conversations or document processing.

So let's do a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">‚ö° Quiz: What is the main benefit of KV Cache?</h3>

<style>
.quiz-container-kvcache { position: relative; }
.quiz-option-kvcache {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-kvcache:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-kvcache { display: none; }
.quiz-radio-kvcache:checked + .quiz-option-kvcache { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-kvcache[value="wrong"]:checked + .quiz-option-kvcache { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-kvcache {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#kvcache-correct:checked ~ .feedback-kvcache.success { opacity: 1; }
#kvcache-wrong1:checked ~ .feedback-kvcache.error, #kvcache-wrong2:checked ~ .feedback-kvcache.error { opacity: 1; }
.feedback-kvcache.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-kvcache.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-kvcache">

  <input type="radio" name="quiz-kvcache" id="kvcache-wrong1" class="quiz-radio-kvcache" value="wrong">
  <label for="kvcache-wrong1" class="quiz-option-kvcache">üìä It improves the quality of generated text</label>

  <input type="radio" name="quiz-kvcache" id="kvcache-wrong2" class="quiz-radio-kvcache" value="wrong">
  <label for="kvcache-wrong2" class="quiz-option-kvcache">üß† It increases the model's context window capacity</label>
  
  <input type="radio" name="quiz-kvcache" id="kvcache-correct" class="quiz-radio-kvcache" value="correct">
  <label for="kvcache-correct" class="quiz-option-kvcache" data-correct="true">üöÄ It avoids recomputing attention for previous tokens, speeding up generation</label>

  <div class="feedback-kvcache success">‚úÖ <strong>Perfect!</strong> KV Cache stores previous computations to avoid redundant calculations during token generation.</div>
  <div class="feedback-kvcache error">‚ùå <strong>Think again!</strong> KV Cache is about performance optimization, not content quality or capacity.</div>
</div>

</div>

## ÔøΩ Prompting: How to Guide the Model

Prompting is how we shape model behavior. There are two key parts:
- **System Prompt**: Defines the assistant‚Äôs role (e.g., ‚ÄúYou are a helpful tutor‚Ä¶‚Äù)
- **User Prompt**: The actual input or question

Small changes in wording can **dramatically** change the output. That‚Äôs why prompt engineering is crucial in education ‚Äî it determines how clearly, accurately, and appropriately the model responds to students and instructors.

üìö We‚Äôll dive much deeper into **prompt engineering strategies**, including real examples and hands-on practice, in the next chapters.
<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üìù Quiz: What's the difference between system and user prompts?</h3>

<style>
.quiz-container-prompting { position: relative; }
.quiz-option-prompting {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-prompting:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-prompting { display: none; }
.quiz-radio-prompting:checked + .quiz-option-prompting { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-prompting[value="wrong"]:checked + .quiz-option-prompting { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-prompting {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#prompting-correct:checked ~ .feedback-prompting.success { opacity: 1; }
#prompting-wrong1:checked ~ .feedback-prompting.error, #prompting-wrong2:checked ~ .feedback-prompting.error { opacity: 1; }
.feedback-prompting.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-prompting.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-prompting">

  <input type="radio" name="quiz-prompting" id="prompting-wrong1" class="quiz-radio-prompting" value="wrong">
  <label for="prompting-wrong1" class="quiz-option-prompting">üîÑ System prompts are user questions and user prompts define the assistant's role</label>

  <input type="radio" name="quiz-prompting" id="prompting-correct" class="quiz-radio-prompting" value="correct">
  <label for="prompting-correct" class="quiz-option-prompting" data-correct="true">üé≠ System prompts define the assistant's role, user prompts are the actual questions or inputs</label>
  
  <input type="radio" name="quiz-prompting" id="prompting-wrong2" class="quiz-radio-prompting" value="wrong">
  <label for="prompting-wrong2" class="quiz-option-prompting">üìä Both system and user prompts serve the same purpose and can be used interchangeably</label>

  <div class="feedback-prompting success">‚úÖ <strong>Perfect!</strong> System prompts establish the AI's behavior and role, while user prompts provide the specific task or question.</div>
  <div class="feedback-prompting error">‚ùå <strong>Try again!</strong> Think about how system prompts set the context and user prompts provide the specific input.</div>
</div>

</div>

## üö® Hallucinations: When Models Make Stuff Up

LLMs sometimes **hallucinate** ‚Äî confidently generate text that‚Äôs incorrect or fictional.

Why it happens:
- They optimize for coherence, not factual accuracy
- They don't ‚Äúknow‚Äù facts‚Äîthey predict likely token sequences

Mitigation tips:
- Include accurate facts in the prompt
- Use guardrails (see below)
- Add retrieval or validation layers

So let's do a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üö® Quiz: Why do LLMs hallucinate?</h3>

<style>
.quiz-container-hallucination { position: relative; }
.quiz-option-hallucination {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-hallucination:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-hallucination { display: none; }
.quiz-radio-hallucination:checked + .quiz-option-hallucination { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-hallucination[value="wrong"]:checked + .quiz-option-hallucination { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-hallucination {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#hallucination-correct:checked ~ .feedback-hallucination.success { opacity: 1; }
#hallucination-wrong1:checked ~ .feedback-hallucination.error, #hallucination-wrong2:checked ~ .feedback-hallucination.error { opacity: 1; }
.feedback-hallucination.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-hallucination.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-hallucination">

  <input type="radio" name="quiz-hallucination" id="hallucination-wrong1" class="quiz-radio-hallucination" value="wrong">
  <label for="hallucination-wrong1" class="quiz-option-hallucination">üêõ It's a bug that will be fixed in future models</label>

  <input type="radio" name="quiz-hallucination" id="hallucination-wrong2" class="quiz-radio-hallucination" value="wrong">
  <label for="hallucination-wrong2" class="quiz-option-hallucination">üíæ They have corrupted training data</label>
  
  <input type="radio" name="quiz-hallucination" id="hallucination-correct" class="quiz-radio-hallucination" value="correct">
  <label for="hallucination-correct" class="quiz-option-hallucination" data-correct="true">üéØ They optimize for coherence and predict likely token sequences, not factual accuracy</label>

  <div class="feedback-hallucination success">‚úÖ <strong>Excellent!</strong> LLMs generate plausible-sounding text based on patterns, not facts they "know" to be true.</div>
  <div class="feedback-hallucination error">‚ùå <strong>Try again!</strong> Think about how LLMs fundamentally work - they predict tokens, not retrieve facts.</div>
</div>

</div>

## üõ°Ô∏è Guardrails: Controlling What Models Say

To keep your assistant safe and on-task, you can apply **guardrails**, such as:
- Prompt templates with strict instructions
- Output filters (block offensive or harmful content)
- External validation (e.g., fact-checking or classifiers)

For Canopy AI, these guardrails are essential to ensure alignment with educational standards.

So let's do a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üõ°Ô∏è Quiz: What are guardrails in AI systems?</h3>

<style>
.quiz-container-guardrails { position: relative; }
.quiz-option-guardrails {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-guardrails:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-guardrails { display: none; }
.quiz-radio-guardrails:checked + .quiz-option-guardrails { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-guardrails[value="wrong"]:checked + .quiz-option-guardrails { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-guardrails {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#guardrails-correct:checked ~ .feedback-guardrails.success { opacity: 1; }
#guardrails-wrong1:checked ~ .feedback-guardrails.error, #guardrails-wrong2:checked ~ .feedback-guardrails.error { opacity: 1; }
.feedback-guardrails.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-guardrails.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-guardrails">

  <input type="radio" name="quiz-guardrails" id="guardrails-wrong1" class="quiz-radio-guardrails" value="wrong">
  <label for="guardrails-wrong1" class="quiz-option-guardrails">üöÄ Performance optimizations that make models run faster</label>

  <input type="radio" name="quiz-guardrails" id="guardrails-correct" class="quiz-radio-guardrails" value="correct">
  <label for="guardrails-correct" class="quiz-option-guardrails" data-correct="true">üõ°Ô∏è Safety mechanisms that control and constrain model behavior and outputs</label>
  
  <input type="radio" name="quiz-guardrails" id="guardrails-wrong2" class="quiz-radio-guardrails" value="wrong">
  <label for="guardrails-wrong2" class="quiz-option-guardrails">üìä Data preprocessing techniques used during model training</label>

  <div class="feedback-guardrails success">‚úÖ <strong>Excellent!</strong> Guardrails are essential safety measures that help ensure AI systems behave appropriately and safely in production environments.</div>
  <div class="feedback-guardrails error">‚ùå <strong>Try again!</strong> Think about how guardrails help control what an AI system can and cannot do or say.</div>
</div>

</div>

## ÔøΩ Key Inference Metrics

Understanding how your model performs helps you scale and troubleshoot.

| Metric                  | Meaning                                                      |
|-------------------------|--------------------------------------------------------------|
| **TTFT**                | Time to First Token ‚Äì how fast the model starts responding   |
| **TPOT**                | Time Per Output Token ‚Äì how fast each new token is generated |
| **Throughput**          | Number of parallel requests handled                          |
| **VRAM Usage**          | GPU memory required (‚Üë model size or context = ‚Üë memory)     |

These metrics help you balance latency vs. cost in OpenShift AI deployments.

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üìè Quiz: What does TTFT measure in LLM inference?</h3>

<style>
.quiz-container-metrics { position: relative; }
.quiz-option-metrics {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-metrics:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-metrics { display: none; }
.quiz-radio-metrics:checked + .quiz-option-metrics { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-metrics[value="wrong"]:checked + .quiz-option-metrics { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-metrics {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#metrics-correct:checked ~ .feedback-metrics.success { opacity: 1; }
#metrics-wrong1:checked ~ .feedback-metrics.error, #metrics-wrong2:checked ~ .feedback-metrics.error { opacity: 1; }
.feedback-metrics.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-metrics.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-metrics">

  <input type="radio" name="quiz-metrics" id="metrics-wrong1" class="quiz-radio-metrics" value="wrong">
  <label for="metrics-wrong1" class="quiz-option-metrics">üíæ The total amount of VRAM memory consumed during inference</label>

  <input type="radio" name="quiz-metrics" id="metrics-wrong2" class="quiz-radio-metrics" value="wrong">
  <label for="metrics-wrong2" class="quiz-option-metrics">üìä The number of tokens processed per second during generation</label>
  
  <input type="radio" name="quiz-metrics" id="metrics-correct" class="quiz-radio-metrics" value="correct">
  <label for="metrics-correct" class="quiz-option-metrics" data-correct="true">‚ö° How long it takes for the model to generate its first response token</label>

  <div class="feedback-metrics success">‚úÖ <strong>Perfect!</strong> TTFT (Time to First Token) measures the initial latency before the model starts responding, which is crucial for user experience.</div>
  <div class="feedback-metrics error">‚ùå <strong>Not quite!</strong> TTFT specifically measures the time delay before the model produces its first output token.</div>
</div>

</div>

## üì¶ Model Sizes and GPU Needs

Model size matters‚Äîfor performance *and* capability.

| Model Size     | Parameters | GPU Requirement            | Notes                            |
|----------------|------------|-----------------------------|----------------------------------|
| **<3B**         | Small      | 8‚Äì12GB VRAM (1 GPU)         | Lightweight and fast             |
| **7B‚Äì13B**      | Medium     | ‚â•24GB VRAM or quantization  | Balanced power vs. cost          |
| **>30B**        | Large      | Multi-GPU or high-end cards | Slower, but more context-aware   |

üß† Larger models may be smarter, but smaller ones are often faster and easier to deploy.

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">üì¶ Quiz: What's the trade-off with model sizes?</h3>

<style>
.quiz-container-models { position: relative; }
.quiz-option-models {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-models:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-models { display: none; }
.quiz-radio-models:checked + .quiz-option-models { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-models[value="wrong"]:checked + .quiz-option-models { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-models {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#models-correct:checked ~ .feedback-models.success { opacity: 1; }
#models-wrong1:checked ~ .feedback-models.error, #models-wrong2:checked ~ .feedback-models.error { opacity: 1; }
.feedback-models.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-models.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-models">

  <input type="radio" name="quiz-models" id="models-wrong1" class="quiz-radio-models" value="wrong">
  <label for="models-wrong1" class="quiz-option-models">üìà Larger models are always better and should be chosen when possible</label>

  <input type="radio" name="quiz-models" id="models-wrong2" class="quiz-radio-models" value="wrong">
  <label for="models-wrong2" class="quiz-option-models">‚ö° Model size doesn't affect hardware requirements</label>
  
  <input type="radio" name="quiz-models" id="models-correct" class="quiz-radio-models" value="correct">
  <label for="models-correct" class="quiz-option-models" data-correct="true">‚öñÔ∏è Larger models may be more capable but require more GPU resources and are slower</label>

  <div class="feedback-models success">‚úÖ <strong>Great understanding!</strong> Model selection involves balancing capability with resource constraints and performance needs.</div>
  <div class="feedback-models error">‚ùå <strong>Not quite!</strong> Consider the trade-offs between model capability and deployment requirements.</div>
</div>

</div>

## ‚úÖ Summary

| Concept                | Key Idea                                                            |
|------------------------|---------------------------------------------------------------------|
| **Token**              | The basic unit of LLM input/output                                  |
| **Next-token machine** | LLMs predict one token at a time                                    |
| **Attention**          | Helps models focus on relevant words                                |
| **Context length**     | How much the model can "remember"                                   |
| **KV Cache**           | Speeds up generation by caching internal state                      |
| **Prompting**          | Guides model behavior through smart input design                    |
| **Hallucination**      | LLMs can generate plausible but wrong info                          |
| **Guardrails**         | Techniques to constrain model behavior and output                   |
| **TTFT & TPOT**        | Speed metrics for user experience                                   |
| **VRAM & Throughput**  | Resource and scalability metrics                                    |
| **Model size & GPU**   | Match model size to hardware capability and use case                |

---

## üìö References

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information processing systems*, 30. This seminal paper introduced the Transformer architecture and self-attention mechanism that forms the foundation of modern large language models like GPT, BERT, and others used in AI applications today.

