# üìä Metrics: Measuring What Matters

Think of metrics like a teacher's gradebook combined with attendance records. A gradebook doesn't just tell you "students learned something" - it shows **how much**, **how consistently**, and **how the trend changes over time**. 

Metrics do the same for Canopy: not just "is it working?" but "how well is it working, and is that improving or degrading?". Let's see what Canopy is already telling us about its performance.

## Exploring vLLM Metrics in RHOAI Observability Stack

Before diving into Canopy's specific application metrics, it's important to understand the infrastructure powering your AI assistant. In this lab environment, **all users share a single llama-3.2 model** deployed in the `ai501` namespace. This shared model serves as the LLM backend for everyone's CanopyUI applications.

<!-- Why share a model? Running large language models requires significant GPU resources. By deploying one shared inference service, the lab environment can support many students simultaneously without requiring dedicated GPUs for each user. Your Canopy application sends requests to this shared vLLM endpoint, which processes them and returns generated text. -->

The vLLM inference engine powering this shared model automatically exports metrics about token generation, request processing, and model performance. These metrics flow into Prometheus automatically as the model is part of OpenShift AI's managed infrastructure.

> If you're curious about the technical details, you can see the InferenceService configuration [here](https://github.com/rhoai-genaiops/deploy-lab/blob/main/student-content/templates/cloud-model/inferenceservice.yaml#L33) - this defines how the llama-3.2 model is deployed and exposed.

### Exploring vLLM Metrics in RHOAI Prometheus

You already deployed Grafana in the previous section, and Prometheus has been collecting metrics from the shared vLLM model this whole time. Let's query them directly.

1. Open the OpenShift Metrics Dashboard for the `ai501` namespace directly [here](https://console-openshift-console.<CLUSTER_DOMAIN>/dev-monitoring/ns/ai501/metrics).

2. In the query box, enter this PromQL query to see total tokens generated by the shared model:

   ```promql
   vllm:generation_tokens_total{namespace="ai501"}
   ```

   ![Obsv 1](./images/obs1.png)

   This shows cumulative tokens the shared llama-3.2 model has generated across all users - a proxy for how much AI assistance the entire lab is providing.

   _Note: You may see an "Access restricted" warning - this is cosmetic and doesn't affect the metric query._

3. Check the shared model's request success rate with this query:

   ```promql
   rate(vllm:request_success_total{namespace="ai501"}[5m])
   ```

   ![Obsv 2](./images/obs2.png)

   This shows successful requests per second for the shared inference service. A sudden drop indicates something's wrong with the model or infrastructure affecting all users.

## Visualizing Metrics in Grafana

While Prometheus queries are powerful for investigation, Grafana dashboards make metrics accessible to everyone on your team. No one wants to write PromQL just to check if the system is healthy!

The Grafana instance you deployed includes several pre-configured dashboards for monitoring your complete AI stack: the shared vLLM model, your Canopy UI, your Canopy Backend and LlamaStack.

### Dashboard 1: vLLM Shared Model Performance

This dashboard shows the health and performance of the shared llama-3.2 inference service that all users depend on. Unlike traditional web applications that focus on request count and response time, LLM systems require specialized metrics that reflect the unique characteristics of AI workloads.

1. Navigate to Grafana - you can use the Quick Links dropdown in OpenShift, or run this command to get the URL:

   ```bash
   echo https://$(oc get route canopy-grafana-route --template='{{ .spec.host }}' -n <USER_NAME>-toolings)
   ```

2. Log in with your OpenShift credentials and click **Allow selected permissions**.

3. Go to **Dashboards ‚Üí Browse** and look for dashboards in the `<USER_NAME>-toolings Canopy Dashboards` folder.

4. Open the **vLLM AI501 Dashboard** to see AI-specific performance metrics:

> This dashboard update in real-time. Since the vLLM model is shared, you'll see activity from all users in the lab. This is normal - you're observing the collective workload on the inference service.

   ![Metrics Dashboard](./images/metrics2.1.png)

   **Token Throughput Metrics:**
   - **Generation Token Throughput (TPS)**: How many tokens per second the model is generating. This measures how fast the response streams - higher TPS means faster completion of answers. During peak hours, you want to ensure TPS doesn't degrade. This is fundamentally different from traditional "requests per second" because it measures the model's text generation speed, not just API throughput.
   - **Prompt Token Throughput**: How many prompt tokens per second are being processed (indicates request volume and context size trends)
   - **Scheduler State**: Number of requests running, waiting, or swapped. High wait queues indicate the shared model is overloaded - a critical signal that doesn't exist in traditional stateless APIs.

   ![Metrics Dashboard](./images/metrics2.2.png)

   **Request Characteristics - Understanding Context:**
   - **Request Prompt Length**: Heatmap showing distribution of prompt sizes. How much conversation history are we maintaining? Longer contexts provide better answers but consume more memory and slow down inference.
   - **Request Generation Length**: Heatmap showing distribution of response sizes

   **Resource and Scheduler Metrics:**
   - **KV Cache Utilization**: Memory usage for attention caching (high values may slow inference). This is unique to transformer models where the "K" (key) and "V" (value) matrices from the attention mechanism are cached to avoid recomputation.
   - **Successfully Processed Requests**: Breakdown by finish reason (EOS token vs max length reached)

   **Latency Metrics - The AI User Experience:**
   - **E2E Request Latency**: End-to-end time from request to completion (p50, p90, p95, p99 percentiles). Traditional web apps might measure this alone, but for LLMs we need to break it down further.
   - **Time To First Token (TTFT)**: How quickly the model starts responding. This affects perceived responsiveness
   - **Time Per Output Token**: Inter-token latency during generation (affects streaming speed and determines how smoothly text appears)

<!-- **Why These AI-Specific Metrics Matter:** Beyond basic infrastructure metrics (CPU, memory), these measurements reflect the unique characteristics of LLM workloads. They distinguish monitoring an AI system from monitoring a traditional web application - your Canopy backend might have great API response times, but if the underlying model has poor TTFT or low TPS, the user experience suffers. -->

### Dashboard 2: Canopy-UI Metrics

Your Canopy UI is the student-facing interface - the web application where users ask questions and receive answers. This dashboard shows how well the frontend is performing.

> For your Canopy application components (UI and [Backend](https://github.com/rhoai-genaiops/backend/blob/main/chart/templates/deployment.yaml#L17)), metrics collection requires an additional label. The Canopy Helm charts have already been configured with `monitoring.opendatahub.io/scrape: 'true'` in their deployment templates, which tells Prometheus to scrape metrics from these workloads. This label is essential for custom applications, without it, Red Hat OpenShift AI Observability stack won't collect their metrics even if they expose them.

1. In Grafana, navigate to **Dashboards ‚Üí Browse ‚Üí `<USER_NAME>-toolings Canopy Dashboards`**

2. Open the **Canopy UI Dashboard** to see:

   ![Metrics Dashboard](./images/metrics3.png)

   **High-Level Stats (Top Row):**
   - **Total Requests (Last Hour)**: How many HTTP requests your UI has served
   - **Average Response Time**: p50 latency for UI responses (should be < 500ms for good UX)
   - **Error Rate (%)**: Percentage of failed requests (should be < 1%)
   - **Active Requests**: Number of requests currently being processed

   **Request Patterns:**
   - **Request Rate (req/sec)**: Real-time view of traffic to your UI
   - **Response Time Percentiles**: p50, p95, p99 latencies over time (watch for spikes)

   **Health Indicators:**
   - **HTTP Status Codes**: Distribution of 2xx, 4xx, 5xx responses
   - **Backend Request Duration**: How long calls from UI to backend are taking (p95)

The UI dashboard helps you understand the student experience. High latencies or error rates here mean students are having a poor experience, even if the backend and model are healthy.

### Dashboard 3: Canopy-Backend Metrics

Your Canopy Backend is the API layer that orchestrates calls between the UI and LlamaStackThis dashboard reveals backend performance and bottlenecks.

1. In the same Grafana folder, open the **Canopy Backend Dashboard** to see:

   **High-Level Stats (Top Row):**
   - **Total API Requests (Last Hour)**: Volume of backend API calls
   - **API Response Time (p95)**: 95th percentile latency for backend endpoints
   - **Error Rate (%)**: Percentage of failed API calls
   - **Active Requests**: Current backend request queue depth

   ![Metrics Dashboard](./images/metrics3.1.png)

   **Endpoint Performance:**
   - **Request Rate by Endpoint**: Which API endpoints are getting the most traffic
   - **Response Time by Endpoint (p95)**: Latency breakdown per endpoint (helps identify slow APIs)
   - **Endpoint Performance Summary**: Table view with request rate, latency, and error rate per endpoint

   **External Service Metrics:**
   - **LLM Call Duration (Outbound to llamastack)**: How long calls to the shared vLLM model are taking (p50, p95, p99)
   - **HTTP Status Distribution**: Success vs. error responses from the backend

   ![Metrics Dashboard](./images/metrics3.2.png)

The backend dashboard is crucial for debugging performance issues.

### Dashboard 4: LlamaStack Token Metrics

LlamaStack sits between your Canopy Backend and the vLLM model, orchestrating LLM calls and tracking token usage. This dashboard shows how efficiently you're using LLM tokens.

**How These Metrics Are Collected**: LlamaStack uses OpenTelemetry's push model to export metrics. The LlamaStack SDK logs token usage during inference, batching events and sending them to the RHOAI Prometheus collector every 60 seconds via OTLP. This differs from Prometheus scraping and allows sending metrics to any OTEL-compatible backend without Prometheus-specific instrumentation.

> **üìä Why Metrics May Differ from vLLM**: LlamaStack metrics track your application's token usage, while vLLM metrics show the shared infrastructure view. Since multiple users share the vLLM model in `ai501`, the dashboards show different perspectives - your app's consumption vs. total infrastructure load.

1. In the same Grafana folder, open the **LlamaStack Metrics Dashboard** to see:

   ![Metrics Dashboard](./images/metrics4.png)

   **Token Usage Overview (Top Row):**
   - **Total Tokens Processed (Last Hour)**: Combined prompt + completion tokens
   - **Prompt Tokens (Last Hour)**: Input tokens sent to the model
   - **Completion Tokens (Last Hour)**: Output tokens generated by the model
   - **Token Rate (tokens/sec)**: Real-time token processing throughput

   **Token Flow Analysis:**
   - **Token Processing Rate Over Time**: Separate lines for prompt, completion, and total token rates showing real-time throughput

   **Cumulative and Efficiency Metrics:**
   - **Cumulative Token Usage**: Growing total of all tokens processed since deployment
   - **Token Usage by Model Summary**: Table breaking down token metrics per model and provider

> **‚ö†Ô∏è Current Limitations**: LlamaStack's metrics telemetry is currently limited to basic token metrics for inference operations only. As documented in [GitHub issue #2596](https://github.com/llamastack/llama-stack/issues/2596), comprehensive observability for other LlamaStack APIs (agents, safety checks, tool runtime, vector I/O, evaluation) is not yet available. The dashboard shows only inference-related token consumption - other LlamaStack operations like RAG queries, agent workflows, or safety filtering are not currently instrumented with metrics.

<!-- ## Understanding the Metrics Flow

Your AI assistant spans four layers, each with its own metrics:

**1. Canopy-UI (`<USER_NAME>-canopy` namespace)** - The student-facing frontend
- Metrics show: request rate, response time, HTTP status codes
- **What it tells you**: Are students having a good experience?

**2. Canopy-Backend (`<USER_NAME>-canopy` namespace)** - The backend
- Metrics show: endpoint performance, outbound call duration, error rates
- **What it tells you**: Where are bottlenecks in your application logic?

**3. LlamaStack (`<USER_NAME>-canopy` namespace)** - The AI orchestration layer
- Metrics show: token usage (prompt/completion), processing rate, model-specific throughput
- **What it tells you**: How efficiently are you using LLM tokens? What's the prompt-to-completion ratio?

**4. Shared vLLM Model (ai501 namespace)** - The inference engine
- Metrics show: token throughput, latency, queue depth, cache usage
- **What it tells you**: Is the shared model healthy and responsive? -->

## üéØ Next Steps: Understanding Behavior with Logs

Metrics tell you **how much** and **how fast**, but not **what's happening** inside your application. When metrics show a problem (success rate dropping, latency spiking), you need details about what went wrong.

For that, we need logs - detailed records of every event in Canopy's operation. Continue to **[Logging](7-observability/3-logging.md)** to learn how to collect and query Canopy's logs üìù
