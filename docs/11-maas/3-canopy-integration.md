# ğŸŒ³ Canopy Integration

> ğŸŒ³ **Persona Focus: Everyone** â€” This is the moment where it all comes together! Canopy has been talking directly to your model endpoint. Time to upgrade it to use MaaS â€” and see the full lifecycle in action.

---

## ğŸ¯ What You'll Learn

In this lesson, you'll:

* ğŸ”€ Switch Canopy from direct model access to MaaS
* âš™ï¸ Configure the necessary environment variables
* ğŸ§ª Test the integration end-to-end
* ğŸ“Š Verify usage tracking in LiteMaaS

---

## ğŸ¬ The Before & After

Let's visualize what we're changing:

### Before (Modules 3-8)

```mermaid
flowchart LR
    subgraph Current["Current Canopy Setup"]
        UI[Canopy UI] --> BE[Canopy Backend]
        BE --> LS[Llama Stack]
        LS --> VLLM[vLLM]
    end
```

### After (Module 11)

```mermaid
flowchart LR
    subgraph MaaSCanopy["MaaS-Connected Canopy"]
        UI[Canopy UI] --> BE[Canopy Backend]
        BE --> LS[Llama Stack]
        LS[Llama Stack] -->|API Key| MAAS[LiteMaaS API]
        MAAS --> VLLM[vLLM]
    end
    subgraph Features
        TRACK[Usage Tracking]
        COST[Cost Attribution]
        SHARED[Shared Model]
    end
    MAAS --> Features
```

Now Canopy goes through LiteMaaS, which provides:
* âœ… Centralized usage tracking
* âœ… Cost attribution
* âœ… API key management
* âœ… Shared model access

---

## ğŸ” Step 1: Create an API Key for Canopy

First, let's create a dedicated API key for Canopy.

### Why a Dedicated Key?

| Approach | Problem |
|----------|---------|
| Use personal key | Can't distinguish Canopy usage from your experiments |
| Share key with team | Who's using what? No accountability |
| **Dedicated app key** | âœ… Clean tracking, easy rotation, specific budget |

1. First, under `Models`, let's subscribe to `Llama-3.2-3B-Instruct-FP8` model. Click it and then hit `Subscribe`

  ![subscribe-llama32-fp8.png](./images/subscribe-llama32-fp8.png)

2. After you successfully subscribed, move to `API Keys` and create an API key for the model you just subscribed!

  ![apikey-canopy.png](./images/apikey-canopy.png)

  It should create a key something like `sk-abcdxxx` 

3. After copying the key, close the screen and click `View Key` to get more information about the API endpoint, and a usage example.

  If you click on `Show key`, the eye ğŸ‘ï¸ icon on right corner, you'll seethat the usage example is updated with your key. Then copy that usage example and paste to your terminal to verify your access to the model endpoint with the generated key.

  ![view-key.png](./images/view-key.png)


## âš™ï¸ Step 2: Update Canopy Configuration

Now we need to tell Llama Stack to use LiteMaaS instead of the direct endpoint.

1. From Llama Stack point of view, it's just another model endpoint that happens to be served via a gateway. Let's add this yet another model endpoint by upgrading the helm release as we've been doing. In `<USER_NAME>-canopy` namespace, go to `Helm` > `Release` > `llama-stack-operator-instance` > Upgrade. Under `models`, click `Add models` and add below info that you get from LiteMaaS:

  name: `Llama-3.2-3B-Instruct-FP8`

  url: `https://litellm-user1-maas.<CLUSTER_DOMAIN>/v1`

  token: `your-token-sk-xxxx`  

  ![maas-canopy](./images/maas-canopy.png)

## ğŸŒ³ Step 3: Update Canopy Backend

Like we've done multiple times before, let's update Canopy backend configuration to use this newly added model to our Llama Stack configuration.

1. Find `canopy-backend` under **OpenShift Console** â†’ **Helm** â†’ **Releases** 


2.  We need to change every `llama32-fp8` into MaaS provided version of that model :) For example, for  `summarize`:

    ```yaml
    summarize:
      enabled: true
      max_tokens: 2048 
      model: vllm-Llama-3.2-3B-Instruct-FP8/Llama-3.2-3B-Instruct-FP8 # ğŸ‘ˆ update this â—ï¸â—ï¸â—ï¸
      prompt: "<your prompt>"
    ```

3. Click **Upgrade** to apply the changes.

4. When everything is blue in Topology view, go to Canopy UI and verify everything works before we repeat this for `test` and `prod` environment, but through GitOps. 

  Actually, before that, let's quickly monitor your model usage based on this key.

---

### ğŸ“Š Step 4: Track Your Usage

As a responsible developer (and budget-conscious human), you'll want to know how much you're using. Go to Canopy UI and send more prompts, requests, and come back to the dashboard to observe the values.

  ![maas-dashboard.png](./images/maas-dashboard.png)

---

## ğŸ§ª Knowledge Check

<details>
<summary>â“ Why should each application have its own API key?</summary>

âœ… **Answer:** Separate API keys provide:
- Clear usage tracking per application
- Individual budgets per application
- Easy key rotation without affecting other apps
- Ability to revoke one app's access without impacting others
</details>

<details>
<summary>â“ What's the benefit of seeing Canopy usage in LiteMaaS?</summary>

âœ… **Answer:** You can now:
- Track how much Canopy costs to operate
- Compare usage across multiple applications
- Set budgets and alerts per API key
- Make data-driven decisions about scaling
- Attribute costs to specific applications and users
</details>

---

### ğŸ‘©â€ğŸ« Step 5: Take it to the Prod!

Well, test first. 

1. Create an API key specifically for test environment, so that we can monitor test environment usage separately or make changes on other environments' keys without impacting this one.

  Since you already susbcribed to the Llama 3.2 3B FP8 before, you directly should go to `API Keys` and create one by giving it a specific name such as `Canopy Test API Key`. 

2. Go back to your workbench and open up `genaiops-gitops/canopy/test/llama-stack/config.yaml`. Again, we first need to make this model available in Llama Stack. Update it by adding the model to the list:

    ```yaml
    ---
    chart_path: charts/llama-stack-operator-instance
    models:
      - name: "llama32"
        url: "http://llama-32-predictor.ai501.svc.cluster.local:8080/v1"
      - name: "llama32-fp8"   
        url: "http://llama-32-fp8-predictor.ai501.svc.cluster.local:8080/v1" 
      - name: "Llama-3.2-3B-Instruct-FP8"     # ğŸ‘ˆ Add this
        url: "https://litellm-user1-maas.<CLUSTER_DOMAIN>/v1" # ğŸ‘ˆ Add this 
        token: `your-token-sk-xxxx`  # ğŸ‘ˆ Update this 
    eval:
      enabled: true
    rag:                  
      enabled: true
    mcp:                
      enabled: true     
    ```

  Yes, you are very right to think _why we are pushing an API key to Git? I don't think this is right!_, and we totally agree with you. We'll come to secret management conversation, promise!

3. Push the changes:

    ```bash
    cd /opt/app-root/src/genaiops-gitops
    git pull
    git add .
    git commit -m "ğŸ„ Add FP8 from MaaS ğŸ„"
    git push
    ```

4. Now let's update the `backend`. Open up `backend/chart/values-test.yaml` and update change every `llama32-fp8` to `Llama-3.2-3B-Instruct-FP8`.

    ```yaml

    LLAMA_STACK_URL: "http://llama-stack-service:8321"
    summarize:
      enabled: true
      model: vllm-Llama-3.2-3B-Instruct-FP8/Llama-3.2-3B-Instruct-FP8 # ğŸ‘ˆ Update this 
      temperature: 0.9
      max_tokens: 4096
      prompt: |
        You are a helpful assistant. Summarize the given text please.
    information-search:
      enabled: true
      vector_db_id: latest
      model: vllm-Llama-3.2-3B-Instruct-FP8/Llama-3.2-3B-Instruct-FP8 # ğŸ‘ˆ Update this 
      prompt: |
        You are a helpful assistant specializing in document intelligence and academic content analysis.
    student-assistant:         
      enabled: true
      model: vllm-Llama-3.2-3B-Instruct-FP8/Llama-3.2-3B-Instruct-FP8 # ğŸ‘ˆ Update this 
      temperature: 0.1
      vector_db_id: latest
      mcp_calendar_url: "http://canopy-mcp-calendar-mcp-server:8080/sse"
      prompt: |
        You are ...
    ```

4. Now let's push the changes:

    ```bash
    cd /opt/app-root/src/backend
    git pull
    git add chart/values-test.yaml
    git commit -m "ğŸ„ Add FP8 from MaaS ğŸ„"
    git push
    ```

    Do you remember what happens when we make a change in the backend? Yes! Evaluation pipeline kicks off! Navigate to OpenShift console > Pipelines > Pipeline Runs under `<USER_NAME>-toolings` namespace and observe the evaluations. 

5. You can follow the same steps for **prod** files to move production Canopy to MaaS as well!
