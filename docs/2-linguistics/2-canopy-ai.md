# ðŸŒ¿ What is Canopy?

<div class="terminal-curl"></div>

**Canopy** is an intelligent, leafy little assistant designed to support teaching and learning at **Redwood Digital University**. From summarizing texts to generating quizzes and scoring assignments â€” itâ€™s your educational AI buddy in action.

Canopy frontend gives you a clean, playful UI built in Streamlit, powered by your choice of LLMs. Whether hosted in OpenShift or running locally, itâ€™s built for experimentation and enablement.

## ðŸŽ¯ Why This Frontend Matters for Prompt Engineering

Just like a good prompt shapes a great model response, **a good user interface shapes great exploration**.

In GenAI applications, **how people interact with the model often matters more than which model you use**.

You can have the smartest LLM in the world, but if the UI doesnâ€™t help users to easily interract with it â€” the value is lost.

This first iteration of **Canopy** is built to support:

- System prompts ðŸ§  to define model behavior.

- User prompts ðŸ’¬ to define what you ask.

- Live streaming output ðŸŒ± so you see each token bloom.

In future modules, this same interface will evolve to handle content creation, grading, and personalized feedback.

## ðŸš€ Getting Started with Canopy on OpenShift

Let's get you your own instance of Canopy up and running in just a few minutes.

### ðŸ“¦ 1. Deploy the Frontend to OpenShift

In OpenShift, you have an experimentation environment which is called `<USER_NAME>-canopy`. You'll use this environment to iterate over Canopy, bring in new features, update the frontend when new capabilities arrive, and so on.

1. Go to [OpenShift Console](https://console-openshift-console.<CLUSTER_DOMAIN>) and use `Students` login.

    User: `<USER_NAME>`  
    Password: `<PASSWORD>`

    ![openshift-console.png](./images/openshift-console.png)

2. Expand `Helm` section from the left menu, click `Releases` and make sure you are on `<USER_NAME>-canopy` project. Then from the top right select `Create Helm Release`. 

    ![add-helm.png](./images/add-helm.png)

3. Select `Canopy Helm Charts` and click on `Canopy UI` helm chart to deploy.

    ![canopy-helm](./images/canopy-helm.png)

4. Hit `Create` and fill out the values as below:

    - **SYSTEM_PROMPT:** Do you remember the great prompt you came up with in the previous section? Let's paste that here!
    - **MODEL_NAME:** `llama32`
    - **LLM_ENDPOINT:** `https://llama32-ai501.<CLUSTER_DOMAIN>`
  
    Leave the rest as it is for now.

    ![helm-values.png](./images/helm-values.png)

    âœ… This will create:

      - A UBI9-based Streamlit application that sends your chat requests to the LLM
      - A secure OpenShift route for us to access the app outside of the cluster

5. Once the application is successfully running, click on the arrow on the side of the circle to access the Canopy UI ðŸŒ³ðŸŒ³ðŸŒ³

    ![canopy-ui-ocp.png](./images/canopy-ui-ocp.png)


### ðŸ§ª 2. Try the Summarization UI

1. You can copy the text about Canopy from Wikipedia: https://en.wikipedia.org/wiki/Canopy_(biology). And paste it to the application to summarize. So paste the content to the text box, press `Summarize` and then watch the model generate a summary in real-time âœ¨

    ![summarize-with-canopy](./images/summarize-with-canopy.png)

---

âœ… What you have accomplished

   - Deployed the Canopy frontend on OpenShift

   - Connected it to the provided LLM endpoint

   - Used the system prompt to shape the assistant's behavior

   - Understood the relationship between prompting and summarization style
