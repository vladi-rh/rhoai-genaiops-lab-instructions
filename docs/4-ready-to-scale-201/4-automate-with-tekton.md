# Automatically trigger on git changes

Now that we have successfully ran our evaluation pipeline (ðŸŽ‰), we would like it to run automatically everytime we make a change to our tests, prompts, or backend.  
To do this, we can create a Tekton pipeline with a git hook to the relevant repos. This Tekton pipeline will then trigger our evaluation kubeflow pipeline. ðŸ”—

## Update Llama Stack in test

Just like we enabled evaluations for Llama Stack in our `experimentation` envirionment, we need to enable it for our `test` environment.

1. Open up your workbench in the `<USER_NAME>-canopy` namespace.
2. Inside of `genaiops-gitops/canopy/test/llama-stack/config.yaml` add this line:
    ```yaml
    eval:
        enabled: true
    ```

    Your final `config.yaml` should look something like this:

    ```yaml
    chart_path: charts/llama-stack-operator-instance
    eval:
        enabled: true
    ```
3. Let's push the changes for Argo CD to pick it up.

    ```bash
    cd /opt/app-root/src/genaiops-gitops
    git add .
    git commit -m  "ðŸ¤” enable evaluation ðŸ¤”"
    git push 
    ```

Great, now you are all set up!  

## Install Pipeline Server

create `dspa` folder

## Trigger our Kubeflow pipeline through a Tekton pipeline

1. Go to your workbench and clone the repo that contains the Tekton pipeline definition:

    ```bash
    cd /opt/app-root/src
    git clone https://<USER_NAME>:<PASSWORD>@gitea-gitea.<CLUSTER_DOMAIN>/<USER_NAME>/genaiops-evals.git
    ```
    Feel free to look around, the Tekton pipeline definition is inside `charts/pipelines`.


2. Now let's deploy the Tekton pipeline through ArgoCD. Start by running: 

    ```bash
    mkdir /opt/app-root/src/genaiops-gitops/toolings/evaluation-pipeline
    touch /opt/app-root/src/genaiops-gitops/toolings/evaluation-pipeline/config.yaml
    ```
    This will create a config file inside `genaiops-gitops/toolings/evaluation-pipeline`.

3. Open up the `evaluation-pipeline/config.yaml` file and paste the below yaml to config.yaml.

    ```yaml
    repo_url: https://gitea-gitea.apps.<CLUSTER_DOMAIN>/<USER_NAME>/canopy-evals.git
    chart_path: test_pipeline/canopy-tekton-pipeline
    USER_NAME: <USER_NAME>
    kfp:
        baseUrl: http://llama-stack.<USER_NAME>-test.svc.cluster.local:80
        backendUrl: http://canopy-backend.<USER_NAME>-test.svc.cluster.local:8000
    ```

    As you may have noticed, we are pointing our base (llamastack) url and backend url to our test namespace, as that's what we want to run our tests on.

4. And finally commit and push it to git, as it only counts if it's in git ðŸ˜‰

    ```bash
    cd /opt/app-root/src/genaiops-gitops
    git add .
    git commit -m "ðŸš„ Evaluation Pipelines ðŸš„"
    git push
    ```

5. Now let's look at it by going to the OpenShift Dashboard -> Pipelines -> user<USER_NAME>-toolings -> `canopy-test-pipeline`. You can see that all it does is a simple git clone followed by starting the kubeflow pipeline.  

    ![tekton-pipeline](images/tekton-pipeline.png)

6. Great, we have our pipeline! However, so far we would still need to trigger it manually, the only difference from before is that we now trigger a Tekton pipeline that then triggers our Kubeflow pipeline and nothing more...

    ![super-important-meme](images/super-important-meme.jpg)

    To get some use of our Tekton pipeline, let's make it trigger automatically from our git repos.  
    Start by going to Gitea.

7. Inside of Gitea, navigate to your `canopy-evals` repository.

8. Go to Settings -> Webhooks

9. Click `Add` and choose Gitea

10. Enter `http://el-canopy-test-event-listener.<USER_NAME>-toolings.svc.cluster.local:8080` -> click Add

    ![githook](images/githook.png)

11. Now we can test if this worked by clicking on `Test webhook connection`.  
    You can go to the pipeline view in OpenShift to see if the pipeline started properly.  

    ![pipeline-started](images/pipeline-started.png)

12. Now do the same for `canopy-backend`.   
    Here we also have a filter in our Trigger so that only changes to the Values.yaml file (in other words the prompts) will trigger the pipeline. If you are interested, you can take a look in `canopy-eval/test_pipeline/canopy-tekton-pipeline/templates/triggers/triggers.yaml`.

13. Whenever the pipeline is ran it produces and saves the results in a MinIO bucket called `test-results`. Go there and see how well your tests performed: `https://minio-ui-<USER_NAME>-toolings.<CLUSTER_DOMAIN>/browser/test-results` 

Congratulations! ðŸŽ‰  
You have now added testing pipelines to your backend and eval repos, so whenever you update your evaluations or prompts, you will run through the tests.  
In practice we would also run the tests whenever we build a new backend, but since we are using pre-built backend images we are skipping that for now.

## Try updating your evaluations

So far, we have only ran the pipeline with predefined tests, let's go and add some useful tests on our own ðŸ§ª

1. Go to your workbench and enter the `canopy-evals/Summary` folder.  
    In there you can find all the tests related to our Summary usecase, specifically inside the file `summary_tests.yaml` which should look something like this:
    ```yaml
    name: summary_tests
    description: Tests for the summary prompts of the Llama 3.2 3B model.
    endpoint: /summarize
    scoring_params:
        "llm-as-judge::base":
            "judge_model": llama32
            "prompt_template": judge_prompt.txt
            "type": "llm_as_judge"
            "judge_score_regexes": ["Answer: (A|B|C|D|E)"]
        "basic::subset_of": null
    tests:
    - prompt: "Llama 3.2 is a state-of-the-art language model that excels in various natural language processing tasks, including summarization, translation, and question answering."
        expected_result: "Llama 3.2 is a top-tier language model for NLP tasks."
    - dataset: "huggingface:small-canopy-qa"
    ```
    Add a couple more tests, here is one example to get you started:
    ```yaml
     - prompt: "Artificial intelligence and machine learning have revolutionized numerous industries in recent years. From healthcare diagnostics that can detect diseases earlier than human doctors, to autonomous vehicles that promise safer transportation, to recommendation systems that personalize our digital experiences, AI technologies are becoming increasingly sophisticated. However, these advances also bring challenges including ethical concerns about bias in algorithms, job displacement due to automation, and the need for robust data privacy protections."
        expected_result: "AI and ML have transformed industries through healthcare diagnostics, autonomous vehicles, and recommendation systems, but also raise concerns about bias, job displacement, and privacy."
    ```

2. After you have finished adding your new tests, commit them to git, watch the pipeline run, and see how the results turn out:
    ```bash
    cd /opt/app-root/src/canopy-evals
    git add .
    git commit -m "ðŸ“– New Evals ðŸ“–"
    git push
    ```